# This is official implementation of our paper: [Accumulated Trivial Attention Matters in Vision Transformers on Small Datasets](https://openaccess.thecvf.com/content/WACV2023/papers/Chen_Accumulated_Trivial_Attention_Matters_in_Vision_Transformers_on_Small_Datasets_WACV_2023_paper.pdf)

Our codes are highly adpated from [SPT_LSA_ViT](https://github.com/aanna0701/SPT_LSA_ViT)
